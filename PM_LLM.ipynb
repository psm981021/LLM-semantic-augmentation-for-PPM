{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c9421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.util.xes_constants import DEFAULT_NAME_KEY, DEFAULT_TIMESTAMP_KEY\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.log.util import dataframe_utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993c3f6",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8919408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Random Forest]\n",
      "Accuracy: 0.8300\n",
      "\n",
      "[Logistic Regression]\n",
      "Accuracy: 0.8300\n",
      "\n",
      "[Decision Tree]\n",
      "Accuracy: 0.8300\n",
      "\n",
      "[SVM]\n",
      "Accuracy: 0.8300\n",
      "\n",
      "[Naive Bayes]\n",
      "Accuracy: 0.8300\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 로드 및 전처리\n",
    "df = pd.read_csv('data/original/Credit.csv')\n",
    "\n",
    "df = df.rename(columns={\n",
    "    'Case': 'case:concept:name',\n",
    "    'Activity': 'concept:name',\n",
    "    'timestamp': 'time:Timestamp'\n",
    "})\n",
    "\n",
    "for col in ['case:concept:name', 'concept:name', 'time:Timestamp']:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "\n",
    "df = dataframe_utils.convert_timestamp_columns_in_df(df)\n",
    "\n",
    "parameters = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case:concept:name'}\n",
    "event_log = log_converter.apply(df, parameters=parameters, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "\n",
    "# 2. prefix 데이터 생성\n",
    "def generate_prefix_data(log, max_prefix_len=5):\n",
    "    data = []\n",
    "    for trace in log:\n",
    "        activities = [event['concept:name'] for event in trace]\n",
    "        for l in range(1, min(len(activities), max_prefix_len)):\n",
    "            prefix = activities[:l]\n",
    "            next_act = activities[l]\n",
    "            data.append((prefix, next_act))\n",
    "    return data\n",
    "\n",
    "# window length\n",
    "MAX_LEN = 5\n",
    "prefix_data = generate_prefix_data(event_log, max_prefix_len=5)\n",
    "\n",
    "unique_acts = list({act for prefix, label in prefix_data for act in prefix + [label]})\n",
    "act2idx = {act: i for i, act in enumerate(unique_acts)}\n",
    "idx2act = {i: act for act, i in act2idx.items()}\n",
    "\n",
    "def vectorize_prefix(prefix, max_len=4):\n",
    "    vec = [act2idx[act] for act in prefix]\n",
    "    vec = vec + [-1] * (max_len - len(vec))\n",
    "    return vec\n",
    "\n",
    "X = np.array([vectorize_prefix(prefix) for prefix, label in prefix_data])\n",
    "y = np.array([act2idx[label] for prefix, label in prefix_data])\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_encoded = ohe.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "models = [\n",
    "    (\"Random Forest\", RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    (\"Logistic Regression\", LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier(random_state=42)),\n",
    "    (\"SVM\", SVC(random_state=42)),\n",
    "    (\"Naive Bayes\", MultinomialNB())\n",
    "]\n",
    "\n",
    "# 4. Test\n",
    "for name, model in models:\n",
    "    \n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"\\n[{name}]\")\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[{name}] 모델은 에러 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30a6efd",
   "metadata": {},
   "source": [
    "#### Baseline LLM Augmented Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb21ca5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Random Forest] Accuracy: 0.6730\n",
      "[Logistic Regression] Accuracy: 0.6720\n",
      "[Decision Tree] Accuracy: 0.6580\n",
      "[SVM] Accuracy: 0.6870\n",
      "[Naive Bayes] Accuracy: 0.4940\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('data/augmented/Credit_augmented_random_0.3.csv')\n",
    "\n",
    "df = df.rename(columns={\n",
    "    'Case': 'case_id:concept:name',\n",
    "    'Activity': 'concept:name',\n",
    "    'Timestamp': 'time:timestamp'\n",
    "})\n",
    "df = dataframe_utils.convert_timestamp_columns_in_df(df)\n",
    "\n",
    "params = {\n",
    "    log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY:\n",
    "    'case_id:concept:name'\n",
    "}\n",
    "log_orig = log_converter.apply(df[df['Aug']==0], parameters=params,\n",
    "                               variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "log_aug  = log_converter.apply(df[df['Aug']==1], parameters=params,\n",
    "                               variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "\n",
    "# prefix/label\n",
    "def generate_prefix_data(log, max_pre_len):\n",
    "    data = []\n",
    "    for trace in log:\n",
    "        acts = [e['concept:name'] for e in trace]\n",
    "        for l in range(1, min(len(acts), max_pre_len) + 1):\n",
    "            nxt = acts[l] if l < len(acts) else None\n",
    "            if nxt:\n",
    "                data.append((acts[:l], nxt))\n",
    "    return data\n",
    "\n",
    "\n",
    "MAX_LEN = 5\n",
    "prefix_orig = generate_prefix_data(log_orig, MAX_LEN)\n",
    "prefix_aug  = generate_prefix_data(log_aug,  MAX_LEN)\n",
    "\n",
    "# index/padding\n",
    "all_prefixes = prefix_orig + prefix_aug\n",
    "unique_acts = sorted({act for pre, nxt in all_prefixes for act in pre + [nxt]})\n",
    "\n",
    "# PAD=0\n",
    "act2idx = {act: i+1 for i, act in enumerate(unique_acts)}\n",
    "PAD = 0\n",
    "\n",
    "def vectorize_prefix(prefix, max_len=MAX_LEN):\n",
    "    vec = [act2idx[a] for a in prefix]\n",
    "    # right padding\n",
    "    vec += [PAD] * (max_len - len(vec))\n",
    "    return vec\n",
    "\n",
    "#  train/test split\n",
    "\n",
    "X_orig = np.array([vectorize_prefix(p, MAX_LEN) for p, _ in prefix_orig])\n",
    "y_orig = np.array([act2idx[n] for _, n in prefix_orig])\n",
    "\n",
    "X_train_o, X_test, y_train_o, y_test = train_test_split(\n",
    "    X_orig, y_orig, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# train = train + aug\n",
    "X_train = np.vstack([X_train_o,\n",
    "                     np.array([vectorize_prefix(p, MAX_LEN) for p, _ in prefix_aug])])\n",
    "y_train = np.hstack([y_train_o,\n",
    "                     np.array([act2idx[n] for _, n in prefix_aug])])\n",
    "\n",
    "# test\n",
    "X_test  = X_test\n",
    "y_test  = y_test\n",
    "\n",
    "#  One-Hot  encoding\n",
    "ct = ColumnTransformer(\n",
    "    [(f\"pos{i}\",\n",
    "      OneHotEncoder(handle_unknown='ignore'),\n",
    "      [i])\n",
    "     for i in range(MAX_LEN)],\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "X_train_enc = ct.fit_transform(X_train)\n",
    "X_test_enc  = ct.transform(X_test)\n",
    "\n",
    "models = [\n",
    "    (\"Random Forest\",      RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    (\"Logistic Regression\",LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    (\"Decision Tree\",      DecisionTreeClassifier(random_state=42)),\n",
    "    (\"SVM\",                SVC(random_state=42)),\n",
    "    (\"Naive Bayes\",        MultinomialNB())\n",
    "]\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train_enc, y_train)\n",
    "    y_pred = model.predict(X_test_enc)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"[{name}] Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c34e5bc",
   "metadata": {},
   "source": [
    "#### Random Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e2c7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[증강 통계]\n",
      "원본 Activity 수: 26836\n",
      "증강 Activity 수: 10309\n",
      "증강 비율: 38.41%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "dataset = \"BPIC15_1\"\n",
    "df = pd.read_csv(\"BPIC15_1.csv\")\n",
    "df = df.sort_values(by=['Case', 'Timestamp'])\n",
    "aug_ratio = 0.3\n",
    "\n",
    "\n",
    "df = df.dropna(how='all')\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
    "activity_set = sorted(df['Activity'].dropna().astype(str).unique().tolist())\n",
    "\n",
    "case_groups = list(df.groupby('Case'))\n",
    "augmented_rows = []\n",
    "random.seed(42)\n",
    "\n",
    "for case_id, group in case_groups:\n",
    "    group = group.sort_values('Timestamp').reset_index(drop=True)\n",
    "    original_rows = group.to_dict('records')\n",
    "    new_rows = [{**row, \"Aug\": 0, \"augmented_activity\": \"\", \"semantic_reason\": \"\"} for row in original_rows]\n",
    "    \n",
    "    seq_len = len(original_rows)\n",
    "    n_aug = max(1, int(seq_len * aug_ratio))\n",
    "   \n",
    "    aug_choices = random.choices(['add', 'delete', 'swap'], k=n_aug)\n",
    "    aug_indices = random.sample(range(seq_len), k=n_aug)\n",
    "\n",
    "    for op, idx in zip(aug_choices, aug_indices):\n",
    "        if op == 'add':\n",
    "           \n",
    "            Activity = random.choice(activity_set)\n",
    "            t_cur = original_rows[idx]['Timestamp']\n",
    "            t_next = original_rows[idx+1]['Timestamp'] if idx+1 < seq_len else t_cur + timedelta(minutes=1)\n",
    "            t_new = t_cur + (t_next - t_cur)/2\n",
    "            aug_row = {\n",
    "                \"case\": case_id,\n",
    "                \"Activity\": Activity,\n",
    "                \"resource\": \"\",\n",
    "                \"Timestamp\": t_new,\n",
    "                \"diagnose\": original_rows[idx].get(\"diagnose\", \"\"),\n",
    "                \"Aug\": 1,\n",
    "                \"augmented_activity\": Activity,\n",
    "                \"semantic_reason\": \"Random add\"\n",
    "            }\n",
    "            new_rows.append(aug_row)\n",
    "        elif op == 'delete' and seq_len > 2:\n",
    "\n",
    "            del_row = {\n",
    "                **original_rows[idx],\n",
    "                \"Aug\": 1,\n",
    "                \"augmented_activity\": original_rows[idx]['Activity'],\n",
    "                \"semantic_reason\": \"Random delete (for augmentation only)\",\n",
    "                \"Timestamp\": original_rows[idx]['Timestamp'],\n",
    "                \"resource\": original_rows[idx].get(\"resource\", \"\")\n",
    "            }\n",
    "            new_rows.append(del_row)\n",
    "        elif op == 'swap' and seq_len > 1 and idx < seq_len-1:\n",
    "            \n",
    "            for swap_idx in [idx, idx+1]:\n",
    "                swap_row = {\n",
    "                    **original_rows[swap_idx],\n",
    "                    \"Aug\": 1,\n",
    "                    \"augmented_activity\": original_rows[idx+1 if swap_idx == idx else idx]['Activity'],\n",
    "                    \"semantic_reason\": \"Random swap\",\n",
    "                    \"Timestamp\": original_rows[swap_idx]['Timestamp'],\n",
    "                    \"resource\": original_rows[swap_idx].get(\"resource\", \"\")\n",
    "                }\n",
    "                new_rows.append(swap_row)\n",
    "    \n",
    "    augmented_rows.extend(new_rows)\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(augmented_rows)\n",
    "result_df = result_df.sort_values(by=['Case', 'Timestamp']).reset_index(drop=True)\n",
    "result_df['Timestamp'] = result_df['Timestamp'].astype(str)\n",
    "\n",
    "result_df.to_csv(f\"{dataset}_augmented_random_{aug_ratio}.csv\", index=False)\n",
    "\n",
    "num_original = (result_df['Aug'] == 0).sum()\n",
    "num_augmented = (result_df['Aug'] == 1).sum()\n",
    "augmentation_rate = num_augmented / num_original\n",
    "\n",
    "print(f\"\\n[증강 통계]\")\n",
    "print(f\"원본 Activity 수: {num_original}\")\n",
    "print(f\"증강 Activity 수: {num_augmented}\")\n",
    "print(f\"증강 비율: {augmentation_rate:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425cb25",
   "metadata": {},
   "source": [
    "#### PM4PY Stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb00230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== End Activities ===\n",
      "Release A: 392\n",
      "Return ER: 291\n",
      "LacticAcid: 3\n",
      "Leucocytes: 5\n",
      "Release B: 51\n",
      "Release E: 5\n",
      "Release C: 18\n",
      "Release D: 14\n",
      "IV Antibiotics: 1\n",
      "Admission NC: 8\n",
      "CRP: 4\n",
      "IV Liquid: 3\n",
      "\n",
      "=== Start Activities ===\n",
      "ER Registration: 752\n",
      "IV Liquid: 13\n",
      "Leucocytes: 16\n",
      "ER Sepsis Triage: 5\n",
      "ER Triage: 4\n",
      "CRP: 5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "# case 통계\n",
    "# end activity\n",
    "from pm4py.statistics.end_activities.log import get as end_activities_get\n",
    "# start activity\n",
    "from pm4py.statistics.start_activities.log import get as start_activities_get\n",
    "\n",
    "\n",
    "df = pd.read_csv('sepsis.csv')\n",
    "df = df.rename(columns={\n",
    "    'case_id': 'case:concept:name',\n",
    "    'activity': 'concept:name',\n",
    "    'timestamp': 'time:timestamp'\n",
    "})\n",
    "\n",
    "for col in ['case:concept:name', 'concept:name', 'time:timestamp']:\n",
    "    if col not in df.columns:\n",
    "        df[col] = pd.NA\n",
    "\n",
    "df = dataframe_utils.convert_timestamp_columns_in_df(df)\n",
    "parameters = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case:concept:name'}\n",
    "event_log = log_converter.apply(df, parameters=parameters, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "\n",
    "\n",
    "# (B) End activities\n",
    "end_activities = end_activities_get.get_end_activities(event_log)\n",
    "print(\"\\n=== End Activities ===\")\n",
    "for act, cnt in end_activities.items():\n",
    "    print(f\"{act}: {cnt}\")\n",
    "\n",
    "# (C) Start activities\n",
    "start_activities = start_activities_get.get_start_activities(event_log)\n",
    "print(\"\\n=== Start Activities ===\")\n",
    "for act, cnt in start_activities.items():\n",
    "    print(f\"{act}: {cnt}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
